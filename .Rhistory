updateSelectInput(session, "agente", choices = filtered_agente)
})
observeEvent(input$predict, {
new_data <- tibble(
tiempo = input$tiempo,
n_fiscalizadores_por_cometido = input$n_fiscalizadores_por_cometido,
medida = input$medida,
agente = input$agente,
especierod = input$especierod,
region = input$region,
acciones_por_cometido = input$acciones_por_cometido,
oficina = input$oficina,
mes = input$mes
)
prediction <- predict(rf_fit, new_data, type="prob")
prediction_percentage <- (prediction[1,2])*100
# Display using renderUI to have more control over appearance
output$prediction <- renderUI({
tagList(
h3("Probabilidad de Encontrar Una InfracciÃ³n:"),
tags$div(
class = "progress",
tags$div(
class = "progress-bar",
role = "progressbar",
style = sprintf("width: %s%%;", prediction_percentage),
sprintf("%s%%", round(prediction_percentage))
)
)
)
})
})
}
shinyApp(ui = ui, server = server)
library(ranger)
`La+Pesca+Ilegal+.+2nd+Version_September+26%2C+2023_11.37` <- read.csv("~/Desktop/La+Pesca+Ilegal+-+2nd+Version_September+26%2C+2023_11.37.csv")
View(`La+Pesca+Ilegal+.+2nd+Version_September+26%2C+2023_11.37`)
library(shiny)
library(lubridate)
library(tidymodels)
library(shiny)
?observe
detach("package:infer", unload = TRUE)
# Load necessary libraries
library(tune)
library(yardstick)
library(dials)
# Define the model specification with tune() placeholders for the parameters to be tuned
rf_model <- rand_forest(
trees = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("ranger", class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
pacman::p_load(
janitor,
tidymodels,
tidyverse,
StanHeaders,
ranger,
lubridate
)
setwd("~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp")
raw_data <- read.csv("Binomial_dataset2.csv")      %>% # Modify here to read from correct folder
clean_names()
# Trim observations and build a factor version for classification
raw_data$tiempo = raw_data$tiempo/60
illegal <- raw_data %>%
drop_na() %>%                                                                 # Remove all observations with NA
filter(!agente == "Sin_Actividad") %>%                                        # Remove agente sin actividad
mutate(f_respuesta = as.factor(ifelse(respuesta == 1, "Si", "No")),           # Recode 0 and 1 to a factor
ano = as.factor(ano))                                                  # Recode year as factor
##Get week
AA=parse_date_time(illegal$fecha, orders   = c('dmy HM', 'Ymd','dmY HM' , 'dmY HMS', 'Ymd HMS'))
BB = format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%Y/%m/%d')
illegal$semana<- week(BB)
##Get day of the week
illegal$dia <-wday(BB)
#write.csv(illegal, "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/illegal.csv")
# Build training (70%) and testing (30%) data sets
set.seed(1)                                                                     # Random seed
illegal_split <- initial_split(data = illegal,
prop = 0.7,                                     # Proportion dstined for training
strata = especie)                             # Stratified sampling to make sure we get 70% of each species. Just as good practice
illegal_train <- training(illegal_split)                                        # Extract the training set
illegal_test  <-  testing(illegal_split)                                        # Extract the testing set
## BUILD ML PIPELINE -----------------------------------------------------------
# Define the "recipe", made up of the formulation and data transformations
illegal_rec <-
recipe(
# Formula relating the (factor)response to relevant variables used in the hierarchical bayesian model
formula = f_respuesta ~ tiempo + n_fiscalizadores_por_cometido +  agente + especie + region  + oficina + semana + dia,
data = illegal_train) %>%
# Add a step to transfomr "tiempo" and "n_fiscalizadores_por_cometido" to be between 0 and 1
#step_range(min = 0,
#          max = 1,
#         ranges = c("tiempo", "n_fiscalizadores_por_cometido")) #%>%
# Add a step to allow for species to change in the future (e.g. if the prediction is made for a species not included here)
step_novel(especie)
# Define the model specification
rf_model <- rand_forest(trees =250, min_n = 5, mtry = NULL) %>%
set_engine("ranger", class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
workflow <-
workflow() %>%
add_model(rf_model) %>%                                                       # Adds the model as defined
add_recipe(illegal_rec)                                                       # Adds the recipe we defined
# Fit the model ----------------------------------------------------------------
rf_fit <- fit(
object = workflow,                                                            # What to fit?
data = illegal_train                                                          # What to fit it to
)
# See model output
rf_fit
#saveRDS(rf_fit, file = "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/rf_fit_model.rds", compress = "xz")
# Load necessary libraries
library(tune)
library(yardstick)
library(dials)
# Define the model specification with tune() placeholders for the parameters to be tuned
rf_model <- rand_forest(
trees = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("ranger", class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
workflow <-
workflow() %>%
add_model(rf_model) %>%                                                       # Adds the model as defined
add_recipe(illegal_rec)                                                       # Adds the recipe we defined
# Create a grid of hyperparameters to search over
rf_grid <- grid_regular(
trees(range = c(200, 500)),
min_n(range = c(1, 5)),
mtry(range = c(1, 3))
# levels = c(10, 20, 30)
)
# Define the resampling method - using 5-fold cross-validation here as an example
cv_folds <- vfold_cv(illegal_train, v = 5)
# Define a metric to optimize - F1 Score
metric <- metric_set(f_meas)
# Perform the grid search
rf_tune_results <- tune_grid(
object = workflow,  # Your existing workflow
resamples = cv_folds,  # Resampling method
grid = rf_grid,  # Grid of hyperparameters
metrics = metric  # Metrics to optimize
)
# View the results
rf_tune_results
# Optionally, you can extract the best set of hyperparameters and re-train your model
best_params <- select_best(rf_tune_results, "f_meas")
rf_model_best <- rf_model %>%
finalize_workflow(best_params)
# View the results
rf_tune_results
# Optionally, you can extract the best set of hyperparameters and re-train your model
best_params <- select_best(rf_tune_results, "f_meas")
best_params
rf_model <- rand_forest(trees =350, min_n = 1, mtry = 1) %>%
set_engine("ranger", class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
# See model output
rf_fit
rf_fit<- rand_forest(trees =350, min_n = 1, mtry = 1) %>%
set_engine("ranger", class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
# See model output
rf_fit
workflow <-
workflow() %>%
add_model(rf_model) %>%                                                       # Adds the model as defined
add_recipe(illegal_rec)                                                       # Adds the recipe we defined
# Fit the model ----------------------------------------------------------------
rf_fit <- fit(
object = workflow,                                                            # What to fit?
data = illegal_train                                                          # What to fit it to
)
# See model output
rf_fit
prob_predictions <- predict(rf_fit, illegal_train, type = "prob")
adjusted_predictions <- ifelse(prob_predictions$.pred_Si > 0.28, "Si", "No")
library(caret)
conf_matrix <- confusionMatrix(as.factor(adjusted_predictions), illegal_train$f_respuesta, mode = "everything", positive="Si")
print(conf_matrix)
results=matrix(0,100,4)
for (cr in 1:100){
crit= cr/100
predicted_violations = ifelse(prob_predictions$.pred_Si >crit, "Si","No")
F1=confusionMatrix(as.factor(predicted_violations), illegal_train$f_respuesta, mode = "everything", positive="Si")
F11=F1$byClass["F1"]
results[cr,1]=crit
results[cr,2]=F11
}
plot(results[,1],results[,2], ylab="F1", xlab="Threshold")
(which.max(results[,2]))/100
results=matrix(0,100,4)
prob_predictions <- predict(rf_fit, illegal_test, type = "prob")
for (cr in 1:100){
crit= cr/100
predicted_violations = ifelse(prob_predictions$.pred_Si >crit, "Si","No")
F1=confusionMatrix(as.factor(predicted_violations), illegal_test$f_respuesta, mode = "everything", positive="Si")
F11=F1$byClass["F1"]
results[cr,1]=crit
results[cr,2]=F11
}
plot(results[,1],results[,2], ylab="F1", xlab="Threshold")
aa=(which.max(results[,2]))/100
adjusted_predictions <- ifelse(prob_predictions$.pred_Si > aa, "Si", "No")
conf_matrix <- confusionMatrix(as.factor(adjusted_predictions), illegal_test$f_respuesta, mode = "everything", positive="Si")
print(conf_matrix)
###I can improve the F1 a lot, from 0.4 to 0.85. But the predictive capacity is only around 0.3 (F1)
pacman::p_load(
janitor,
tidymodels,
tidyverse,
StanHeaders,
ranger,
lubridate
)
setwd("~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp")
raw_data <- read.csv("Binomial_dataset2.csv")      %>% # Modify here to read from correct folder
clean_names()
raw_data <- read.csv("Binomial_dataset2.csv")      %>% # Modify here to read from correct folder
clean_names()
# Trim observations and build a factor version for classification
raw_data$tiempo = raw_data$tiempo/60
illegal <- raw_data %>%
drop_na() %>%                                                                 # Remove all observations with NA
filter(!agente == "Sin_Actividad") %>%                                        # Remove agente sin actividad
mutate(f_respuesta = as.factor(ifelse(respuesta == 1, "Si", "No")),           # Recode 0 and 1 to a factor
ano = as.factor(ano))                                                  # Recode year as factor
##Get week
AA=parse_date_time(illegal$fecha, orders   = c('dmy HM', 'Ymd','dmY HM' , 'dmY HMS', 'Ymd HMS'))
BB = format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%Y/%m/%d')
BB = format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%Y/%m/%d')
illegal$semana<- week(BB)
###Convert cc into the nearest hour, for instance if 08:30 convert to 08:00
illegal$hora <- format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H)
###Convert cc into the nearest hour, for instance if 08:30 convert to 08:00
illegal$hora <- format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H')
format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H:')
###Extract  time of the day (hour) only from AA
cc= format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%H:%M:%S')
format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H:')
format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H:00')
###Convert cc into the nearest hour, for instance if 08:30 convert to 08:00
illegal$hora <- format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H:00')
write.csv(illegal, "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp/illegal.csv")
write.csv(illegal, "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp/illegal.csv")
set.seed(1)                                                                     # Random seed
illegal_split <- initial_split(data = illegal,
prop = 0.7,                                     # Proportion dstined for training
strata = especie)                             # Stratified sampling to make sure we get 70% of each species. Just as good practice
illegal_train <- training(illegal_split)                                        # Extract the training set
illegal_test  <-  testing(illegal_split)                                        # Extract the testing set
## BUILD ML PIPELINE -----------------------------------------------------------
# Define the "recipe", made up of the formulation and data transformations
illegal_rec <-
recipe(
# Formula relating the (factor)response to relevant variables used in the hierarchical bayesian model
formula = f_respuesta ~ tiempo + n_fiscalizadores_por_cometido +  agente + especie + region  + oficina + semana + dia + hora,
data = illegal_train) %>%
# Add a step to transfomr "tiempo" and "n_fiscalizadores_por_cometido" to be between 0 and 1
#step_range(min = 0,
#          max = 1,
#         ranges = c("tiempo", "n_fiscalizadores_por_cometido")) #%>%
# Add a step to allow for species to change in the future (e.g. if the prediction is made for a species not included here)
step_novel(especie)
pacman::p_load(
janitor,
tidymodels,
tidyverse,
StanHeaders,
ranger,
lubridate
)
setwd("~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp")
raw_data <- read.csv("Binomial_dataset2.csv")      %>% # Modify here to read from correct folder
clean_names()
# Trim observations and build a factor version for classification
raw_data$tiempo = raw_data$tiempo/60
illegal <- raw_data %>%
drop_na() %>%                                                                 # Remove all observations with NA
filter(!agente == "Sin_Actividad") %>%                                        # Remove agente sin actividad
mutate(f_respuesta = as.factor(ifelse(respuesta == 1, "Si", "No")),           # Recode 0 and 1 to a factor
ano = as.factor(ano))                                                  # Recode year as factor
##Get week
AA=parse_date_time(illegal$fecha, orders   = c('dmy HM', 'Ymd','dmY HM' , 'dmY HMS', 'Ymd HMS'))
BB = format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%Y/%m/%d')
illegal$semana<- week(BB)
##Get day of the week
illegal$dia <-wday(BB)
###Extract  time of the day (hour) only from AA
cc= format(as.POSIXct(AA, format = '%d/%m/%Y %H:%M:%S'),format = '%H:%M:%S')
### COnvert cc into three time slots: Manana, Tarde, Noche
dd= ifelse(cc>="06:00:00" & cc<="12:00:00", "MaÃ±ana (6:00 - 12:00)", ifelse(cc>="12:00:00" & cc<="18:00:00", "Tarde (12:00-18:00)", "Noche (>18:00)"))
###Convert cc into the nearest hour, for instance if 08:30 convert to 08:00
illegal$hora <- format(as.POSIXct(cc, format = '%H:%M:%S'),format = '%H:00')
write.csv(illegal, "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp/illegal.csv")
# Build training (70%) and testing (30%) data sets
set.seed(1)                                                                     # Random seed
illegal_split <- initial_split(data = illegal,
prop = 0.7,                                     # Proportion dstined for training
strata = especie)                             # Stratified sampling to make sure we get 70% of each species. Just as good practice
illegal_train <- training(illegal_split)                                        # Extract the training set
illegal_test  <-  testing(illegal_split)                                        # Extract the testing set
## BUILD ML PIPELINE -----------------------------------------------------------
# Define the "recipe", made up of the formulation and data transformations
illegal_rec <-
recipe(
# Formula relating the (factor)response to relevant variables used in the hierarchical bayesian model
formula = f_respuesta ~ tiempo + n_fiscalizadores_por_cometido +  agente + especie + region  + oficina + semana + dia + hora,
data = illegal_train) %>%
# Add a step to transfomr "tiempo" and "n_fiscalizadores_por_cometido" to be between 0 and 1
#step_range(min = 0,
#          max = 1,
#         ranges = c("tiempo", "n_fiscalizadores_por_cometido")) #%>%
# Add a step to allow for species to change in the future (e.g. if the prediction is made for a species not included here)
step_novel(especie)
# Define the model specification
rf_model <- rand_forest(trees =250, min_n = 5, mtry = NULL) %>%
set_engine("ranger",importance = "impurity",  # or "permutation" for permutation importance
class.weights = c(Si = 0.9, No = 0.1)) %>%
set_mode("classification")
workflow <-
workflow() %>%
add_model(rf_model) %>%                                                       # Adds the model as defined
add_recipe(illegal_rec)                                                       # Adds the recipe we defined
# Fit the model ----------------------------------------------------------------
rf_fit <- fit(
object = workflow,                                                            # What to fit?
data = illegal_train                                                          # What to fit it to
)
# See model output
rf_fit
#saveRDS(rf_fit, file = "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/rf_fit_model.rds", compress = "xz")
# Extracting the fitted model
model_fit <- rf_fit$fit$fit
# Extracting feature importance
importance <- model_fit$fit$variable.importance
# Converting to data frame for ggplot
importance_df <- data.frame(
Variable = names(importance),
Importance = importance
)
# Sorting by importance
importance_df <- importance_df[order(-importance_df$Importance),]
# Loading library
library(ggplot2)
# Creating a Feature Importance Plot
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Feature Importance Plot",
x = "Variable",
y = "Importance") +
theme_minimal()
saveRDS(rf_fit, file = "~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/rf_fit_model.rds", compress = "xz")
#rsconnect::setAccountInfo(name='perfilesderiesgo',
# token='3532D8313FD973C7F6C3DD342943285E',
#  secret='7rLz7usfFbpZFubLzLBgoOowb9KpL46cWBzWqp8v')# Set working directory and load data and model
#setwd("~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp")
rf_fit <- read_rds("rf_fit_model.rds")
illegal = read.csv("illegal.csv")
species = read.csv("species.csv")
# UI component of the Shiny app
ui <- fluidPage(
titlePanel("App PredicciÃ³n Infracciones"),
tags$img(src='logos.png', height=200, width=600),
sidebarLayout(
sidebarPanel(
dateInput("input_date", "Fecha a Realizar el Cometido:", Sys.Date(), format = "dd/mm/yyyy"),
selectInput("input_hora", "Hora a Realizar el Cometido:", choices = unique(illegal$hora)),
selectInput("region", "Region:", choices = unique(illegal$region)),
selectInput("oficina", "Oficina:", choices = ""),
selectInput("especie", "Especie:", choices = ""),
selectInput("agente", "Agente:", choices = ""),
sliderInput("tiempo", "DuraciÃ³n del Cometido (horas):", min = 1, max = 14, value = 1),
sliderInput("n_fiscalizadores_por_cometido", "NÃºmero Fiscalizadores en el Cometido:", min = 1, max = 7, value = 1),
actionButton("predict", "Predecir")
),
mainPanel(
uiOutput("prediction"),
uiOutput("species_av"),
uiOutput("prediction_category")
)
)
)
# Server component of the Shiny app
server <- function(input, output, session) {
# Update office choices based on selected region
shiny :: observe({
filtered_offices <- illegal %>%
filter(region == input$region) %>%
distinct(oficina) %>%
pull(oficina)
updateSelectInput(session, "oficina", choices = filtered_offices)
})
# Update species choices based on selected office
shiny :: observe({
filtered_species <- illegal %>%
filter(oficina == input$oficina) %>%
distinct(especie) %>%
pull(especie)
updateSelectInput(session, "especie", choices = filtered_species)
})
# Update medida choices based on selected species
shiny :: observe({
filtered_agente <- illegal %>%
filter(especie == input$especie) %>%
distinct(agente) %>%
pull(agente)
updateSelectInput(session, "agente", choices = filtered_agente)
})
# Prediction logic
observeEvent(input$predict, {
showModal(modalDialog(
div(style = "text-align: center",
em(strong("Realizando PredicciÃ³n ..."))),
footer = NULL,
size = "s",
fade = TRUE
))
# Calculate week number from input date
semana <- week(input$input_date)
dia    <- wday(input$input_date)
new_data <- tibble(
tiempo = input$tiempo,
n_fiscalizadores_por_cometido = input$n_fiscalizadores_por_cometido,
medida = input$medida,
agente = input$agente,
especie = input$especie,
region = input$region,
oficina = input$oficina,
semana = semana,
dia = dia,
hora = input$hora
)
prediction <- predict(rf_fit, new_data, type="prob")
prediction_percentage <- (prediction[1,2]) * 100
aa = input$especie
avg_prob <- subset(species, species$.pred_No == aa)
avg_prob <- avg_prob$.pred_Si
threshold <- avg_prob / 2 # Set this to your desired threshold
if (prediction_percentage > avg_prob + threshold) {
category <- "good"
} else if (prediction_percentage < avg_prob - threshold) {
category <- "poor"
} else {
category <- "regular"
}
# Inside your server function, after categorizing the prediction
output$prediction_category <- renderUI({
if (category == "good") {
color <- "green"
text <- "Cometido Sobre el Promedio de la PesquerÃ­a"
font_size <- "20px"
font_family <- "'Arial', sans-serif"
font_weight <- "bold"
} else if (category == "regular") {
color <- "black"
text <- "Cometido en el Promedio de la PesquerÃ­a"
font_size <- "20px"
font_family <- "'Arial', sans-serif"
font_weight <- "bold"
} else {
color <- "red"
text <- "Cometido Bajo el Promedio de la PesquerÃ­a"
font_size <- "20px"
font_family <- "'Arial', sans-serif"
font_weight <- "bold"
}
tags$div(style = sprintf("color: %s;font-size: %s", color, font_size), text)
})
# Display prediction
output$prediction <- renderUI({
tagList(
h3("Probabilidad de Encontrar una InfracciÃ³n en el Cometido:",  style="font-size: 30px;"),
tags$div(
class = "progress",
tags$div(
class = "progress-bar progress-bar-striped active",
role = "progressbar",
style = sprintf("width: %s%%;", prediction_percentage),
sprintf("%s%%", round(prediction_percentage))
)
)
)
})
output$species_av <- renderUI({
tagList(
h3("Promedio", aa, style="color:black;", style="font-size: 18px;"),
tags$div(
class = "progress",
tags$div(
class = "progress-bar progress-bar-striped active",
role = "progressbar",
style = sprintf("width: %s%%;", avg_prob),
sprintf("%s%%", round(avg_prob))
)
)
)
})
removeModal()
})
}
setwd("~/Dropbox/Back up todo/Sernapesca Project/Models and Data/ML/SernapescaApp")
rf_fit <- read_rds("rf_fit_model.rds")
illegal = read.csv("illegal.csv")
